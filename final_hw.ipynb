{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/runo313/runo313/blob/main/final_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuZEJr-u5CVY",
        "outputId": "a07a1141-107f-47db-8caa-9a82cbd8ca8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   200] loss: 1.897\n",
            "[1,   400] loss: 1.601\n",
            "[1,   600] loss: 1.473\n",
            "[1,   800] loss: 1.408\n",
            "[1,  1000] loss: 1.382\n",
            "[1,  1200] loss: 1.313\n",
            "[1,  1400] loss: 1.293\n",
            "[2,   200] loss: 1.202\n",
            "[2,   400] loss: 1.192\n",
            "[2,   600] loss: 1.133\n",
            "[2,   800] loss: 1.137\n",
            "[2,  1000] loss: 1.084\n",
            "[2,  1200] loss: 1.118\n",
            "[2,  1400] loss: 1.077\n",
            "[3,   200] loss: 1.020\n",
            "[3,   400] loss: 1.016\n",
            "[3,   600] loss: 1.016\n",
            "[3,   800] loss: 1.000\n",
            "[3,  1000] loss: 1.002\n",
            "[3,  1200] loss: 0.961\n",
            "[3,  1400] loss: 0.940\n",
            "[4,   200] loss: 0.949\n",
            "[4,   400] loss: 0.936\n",
            "[4,   600] loss: 0.910\n",
            "[4,   800] loss: 0.931\n",
            "[4,  1000] loss: 0.905\n",
            "[4,  1200] loss: 0.908\n",
            "[4,  1400] loss: 0.899\n",
            "[5,   200] loss: 0.848\n",
            "[5,   400] loss: 0.855\n",
            "[5,   600] loss: 0.872\n",
            "[5,   800] loss: 0.870\n",
            "[5,  1000] loss: 0.865\n",
            "[5,  1200] loss: 0.862\n",
            "[5,  1400] loss: 0.822\n",
            "[6,   200] loss: 0.820\n",
            "[6,   400] loss: 0.818\n",
            "[6,   600] loss: 0.842\n",
            "[6,   800] loss: 0.817\n",
            "[6,  1000] loss: 0.823\n",
            "[6,  1200] loss: 0.815\n",
            "[6,  1400] loss: 0.806\n",
            "[7,   200] loss: 0.765\n",
            "[7,   400] loss: 0.796\n",
            "[7,   600] loss: 0.813\n",
            "[7,   800] loss: 0.762\n",
            "[7,  1000] loss: 0.752\n",
            "[7,  1200] loss: 0.797\n",
            "[7,  1400] loss: 0.806\n",
            "[8,   200] loss: 0.745\n",
            "[8,   400] loss: 0.752\n",
            "[8,   600] loss: 0.761\n",
            "[8,   800] loss: 0.746\n",
            "[8,  1000] loss: 0.755\n",
            "[8,  1200] loss: 0.786\n",
            "[8,  1400] loss: 0.747\n",
            "[9,   200] loss: 0.734\n",
            "[9,   400] loss: 0.725\n",
            "[9,   600] loss: 0.743\n",
            "[9,   800] loss: 0.739\n",
            "[9,  1000] loss: 0.745\n",
            "[9,  1200] loss: 0.702\n",
            "[9,  1400] loss: 0.734\n",
            "[10,   200] loss: 0.699\n",
            "[10,   400] loss: 0.725\n",
            "[10,   600] loss: 0.724\n",
            "[10,   800] loss: 0.713\n",
            "[10,  1000] loss: 0.725\n",
            "[10,  1200] loss: 0.733\n",
            "[10,  1400] loss: 0.718\n",
            "Finished Training. Total training time: 1422.48 seconds\n",
            "Accuracy of the network on the 10000 test images: 77.83%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(self.dropout(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = SimpleCNN()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the network\n",
        "def train():\n",
        "    net.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(10):  # Loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:    # Print every 200 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Finished Training. Total training time: {training_time:.2f} seconds')\n",
        "\n",
        "# Evaluate the network\n",
        "def test():\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f}%')\n",
        "\n",
        "# Run training and testing\n",
        "train()\n",
        "test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "athheo5mHXRH",
        "outputId": "ae495acc-e9fe-46cb-86f7-d7f0c7425720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 76917629.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.098\n",
            "[1,   400] loss: 1.795\n",
            "[1,   600] loss: 1.682\n",
            "[1,   800] loss: 1.580\n",
            "[1,  1000] loss: 1.511\n",
            "[1,  1200] loss: 1.485\n",
            "[1,  1400] loss: 1.445\n",
            "[2,   200] loss: 1.392\n",
            "[2,   400] loss: 1.361\n",
            "[2,   600] loss: 1.344\n",
            "[2,   800] loss: 1.339\n",
            "[2,  1000] loss: 1.270\n",
            "[2,  1200] loss: 1.293\n",
            "[2,  1400] loss: 1.264\n",
            "[3,   200] loss: 1.227\n",
            "[3,   400] loss: 1.215\n",
            "[3,   600] loss: 1.188\n",
            "[3,   800] loss: 1.187\n",
            "[3,  1000] loss: 1.187\n",
            "[3,  1200] loss: 1.165\n",
            "[3,  1400] loss: 1.171\n",
            "[4,   200] loss: 1.119\n",
            "[4,   400] loss: 1.125\n",
            "[4,   600] loss: 1.126\n",
            "[4,   800] loss: 1.112\n",
            "[4,  1000] loss: 1.135\n",
            "[4,  1200] loss: 1.105\n",
            "[4,  1400] loss: 1.103\n",
            "[5,   200] loss: 1.067\n",
            "[5,   400] loss: 1.064\n",
            "[5,   600] loss: 1.073\n",
            "[5,   800] loss: 1.075\n",
            "[5,  1000] loss: 1.036\n",
            "[5,  1200] loss: 1.066\n",
            "[5,  1400] loss: 1.060\n",
            "[6,   200] loss: 1.010\n",
            "[6,   400] loss: 1.054\n",
            "[6,   600] loss: 1.024\n",
            "[6,   800] loss: 1.029\n",
            "[6,  1000] loss: 1.012\n",
            "[6,  1200] loss: 1.026\n",
            "[6,  1400] loss: 0.985\n",
            "[7,   200] loss: 0.981\n",
            "[7,   400] loss: 0.974\n",
            "[7,   600] loss: 1.015\n",
            "[7,   800] loss: 0.980\n",
            "[7,  1000] loss: 1.000\n",
            "[7,  1200] loss: 0.994\n",
            "[7,  1400] loss: 1.007\n",
            "[8,   200] loss: 0.963\n",
            "[8,   400] loss: 0.948\n",
            "[8,   600] loss: 0.975\n",
            "[8,   800] loss: 0.977\n",
            "[8,  1000] loss: 0.977\n",
            "[8,  1200] loss: 0.973\n",
            "[8,  1400] loss: 0.954\n",
            "[9,   200] loss: 0.949\n",
            "[9,   400] loss: 0.953\n",
            "[9,   600] loss: 0.956\n",
            "[9,   800] loss: 0.943\n",
            "[9,  1000] loss: 0.954\n",
            "[9,  1200] loss: 0.933\n",
            "[9,  1400] loss: 0.946\n",
            "[10,   200] loss: 0.916\n",
            "[10,   400] loss: 0.946\n",
            "[10,   600] loss: 0.951\n",
            "[10,   800] loss: 0.910\n",
            "[10,  1000] loss: 0.933\n",
            "[10,  1200] loss: 0.940\n",
            "[10,  1400] loss: 0.923\n",
            "[11,   200] loss: 0.907\n",
            "[11,   400] loss: 0.896\n",
            "[11,   600] loss: 0.932\n",
            "[11,   800] loss: 0.918\n",
            "[11,  1000] loss: 0.923\n",
            "[11,  1200] loss: 0.913\n",
            "[11,  1400] loss: 0.913\n",
            "[12,   200] loss: 0.917\n",
            "[12,   400] loss: 0.897\n",
            "[12,   600] loss: 0.890\n",
            "[12,   800] loss: 0.901\n",
            "[12,  1000] loss: 0.907\n",
            "[12,  1200] loss: 0.934\n",
            "[12,  1400] loss: 0.885\n",
            "[13,   200] loss: 0.909\n",
            "[13,   400] loss: 0.886\n",
            "[13,   600] loss: 0.905\n",
            "[13,   800] loss: 0.902\n",
            "[13,  1000] loss: 0.894\n",
            "[13,  1200] loss: 0.896\n",
            "[13,  1400] loss: 0.888\n",
            "[14,   200] loss: 0.901\n",
            "[14,   400] loss: 0.885\n",
            "[14,   600] loss: 0.878\n",
            "[14,   800] loss: 0.879\n",
            "[14,  1000] loss: 0.864\n",
            "[14,  1200] loss: 0.876\n",
            "[14,  1400] loss: 0.880\n",
            "[15,   200] loss: 0.854\n",
            "[15,   400] loss: 0.864\n",
            "[15,   600] loss: 0.883\n",
            "[15,   800] loss: 0.876\n",
            "[15,  1000] loss: 0.893\n",
            "[15,  1200] loss: 0.877\n",
            "[15,  1400] loss: 0.851\n",
            "[16,   200] loss: 0.883\n",
            "[16,   400] loss: 0.845\n",
            "[16,   600] loss: 0.862\n",
            "[16,   800] loss: 0.840\n",
            "[16,  1000] loss: 0.850\n",
            "[16,  1200] loss: 0.869\n",
            "[16,  1400] loss: 0.867\n",
            "[17,   200] loss: 0.860\n",
            "[17,   400] loss: 0.838\n",
            "[17,   600] loss: 0.841\n",
            "[17,   800] loss: 0.841\n",
            "[17,  1000] loss: 0.862\n",
            "[17,  1200] loss: 0.839\n",
            "[17,  1400] loss: 0.844\n",
            "[18,   200] loss: 0.830\n",
            "[18,   400] loss: 0.861\n",
            "[18,   600] loss: 0.867\n",
            "[18,   800] loss: 0.838\n",
            "[18,  1000] loss: 0.842\n",
            "[18,  1200] loss: 0.842\n",
            "[18,  1400] loss: 0.857\n",
            "[19,   200] loss: 0.845\n",
            "[19,   400] loss: 0.838\n",
            "[19,   600] loss: 0.833\n",
            "[19,   800] loss: 0.849\n",
            "[19,  1000] loss: 0.857\n",
            "[19,  1200] loss: 0.862\n",
            "[19,  1400] loss: 0.843\n",
            "[20,   200] loss: 0.847\n",
            "[20,   400] loss: 0.848\n",
            "[20,   600] loss: 0.828\n",
            "[20,   800] loss: 0.837\n",
            "[20,  1000] loss: 0.830\n",
            "[20,  1200] loss: 0.843\n",
            "[20,  1400] loss: 0.844\n",
            "[21,   200] loss: 0.802\n",
            "[21,   400] loss: 0.844\n",
            "[21,   600] loss: 0.829\n",
            "[21,   800] loss: 0.848\n",
            "[21,  1000] loss: 0.840\n",
            "[21,  1200] loss: 0.805\n",
            "[21,  1400] loss: 0.824\n",
            "[22,   200] loss: 0.789\n",
            "[22,   400] loss: 0.846\n",
            "[22,   600] loss: 0.819\n",
            "[22,   800] loss: 0.807\n",
            "[22,  1000] loss: 0.838\n",
            "[22,  1200] loss: 0.839\n",
            "[22,  1400] loss: 0.814\n",
            "[23,   200] loss: 0.816\n",
            "[23,   400] loss: 0.796\n",
            "[23,   600] loss: 0.833\n",
            "[23,   800] loss: 0.856\n",
            "[23,  1000] loss: 0.814\n",
            "[23,  1200] loss: 0.808\n",
            "[23,  1400] loss: 0.790\n",
            "[24,   200] loss: 0.825\n",
            "[24,   400] loss: 0.821\n",
            "[24,   600] loss: 0.811\n",
            "[24,   800] loss: 0.803\n",
            "[24,  1000] loss: 0.805\n",
            "[24,  1200] loss: 0.821\n",
            "[24,  1400] loss: 0.824\n",
            "[25,   200] loss: 0.784\n",
            "[25,   400] loss: 0.802\n",
            "[25,   600] loss: 0.813\n",
            "[25,   800] loss: 0.803\n",
            "[25,  1000] loss: 0.793\n",
            "[25,  1200] loss: 0.821\n",
            "[25,  1400] loss: 0.847\n",
            "[26,   200] loss: 0.792\n",
            "[26,   400] loss: 0.806\n",
            "[26,   600] loss: 0.810\n",
            "[26,   800] loss: 0.791\n",
            "[26,  1000] loss: 0.801\n",
            "[26,  1200] loss: 0.809\n",
            "[26,  1400] loss: 0.803\n",
            "[27,   200] loss: 0.816\n",
            "[27,   400] loss: 0.796\n",
            "[27,   600] loss: 0.795\n",
            "[27,   800] loss: 0.797\n",
            "[27,  1000] loss: 0.824\n",
            "[27,  1200] loss: 0.790\n",
            "[27,  1400] loss: 0.791\n",
            "[28,   200] loss: 0.775\n",
            "[28,   400] loss: 0.820\n",
            "[28,   600] loss: 0.791\n",
            "[28,   800] loss: 0.793\n",
            "[28,  1000] loss: 0.782\n",
            "[28,  1200] loss: 0.828\n",
            "[28,  1400] loss: 0.780\n",
            "[29,   200] loss: 0.803\n",
            "[29,   400] loss: 0.775\n",
            "[29,   600] loss: 0.792\n",
            "[29,   800] loss: 0.813\n",
            "[29,  1000] loss: 0.839\n",
            "[29,  1200] loss: 0.802\n",
            "[29,  1400] loss: 0.826\n",
            "[30,   200] loss: 0.765\n",
            "[30,   400] loss: 0.795\n",
            "[30,   600] loss: 0.784\n",
            "[30,   800] loss: 0.816\n",
            "[30,  1000] loss: 0.772\n",
            "[30,  1200] loss: 0.779\n",
            "[30,  1400] loss: 0.795\n",
            "[31,   200] loss: 0.775\n",
            "[31,   400] loss: 0.780\n",
            "[31,   600] loss: 0.770\n",
            "[31,   800] loss: 0.774\n",
            "[31,  1000] loss: 0.785\n",
            "[31,  1200] loss: 0.797\n",
            "[31,  1400] loss: 0.806\n",
            "[32,   200] loss: 0.768\n",
            "[32,   400] loss: 0.776\n",
            "[32,   600] loss: 0.783\n",
            "[32,   800] loss: 0.793\n",
            "[32,  1000] loss: 0.759\n",
            "[32,  1200] loss: 0.790\n",
            "[32,  1400] loss: 0.781\n",
            "[33,   200] loss: 0.785\n",
            "[33,   400] loss: 0.765\n",
            "[33,   600] loss: 0.773\n",
            "[33,   800] loss: 0.781\n",
            "[33,  1000] loss: 0.766\n",
            "[33,  1200] loss: 0.776\n",
            "[33,  1400] loss: 0.778\n",
            "[34,   200] loss: 0.772\n",
            "[34,   400] loss: 0.745\n",
            "[34,   600] loss: 0.755\n",
            "[34,   800] loss: 0.793\n",
            "[34,  1000] loss: 0.757\n",
            "[34,  1200] loss: 0.790\n",
            "[34,  1400] loss: 0.781\n",
            "[35,   200] loss: 0.764\n",
            "[35,   400] loss: 0.773\n",
            "[35,   600] loss: 0.766\n",
            "[35,   800] loss: 0.758\n",
            "[35,  1000] loss: 0.809\n",
            "[35,  1200] loss: 0.782\n",
            "[35,  1400] loss: 0.752\n",
            "[36,   200] loss: 0.757\n",
            "[36,   400] loss: 0.787\n",
            "[36,   600] loss: 0.754\n",
            "[36,   800] loss: 0.814\n",
            "[36,  1000] loss: 0.758\n",
            "[36,  1200] loss: 0.771\n",
            "[36,  1400] loss: 0.785\n",
            "[37,   200] loss: 0.776\n",
            "[37,   400] loss: 0.769\n",
            "[37,   600] loss: 0.756\n",
            "[37,   800] loss: 0.772\n",
            "[37,  1000] loss: 0.782\n",
            "[37,  1200] loss: 0.750\n",
            "[37,  1400] loss: 0.774\n",
            "[38,   200] loss: 0.770\n",
            "[38,   400] loss: 0.766\n",
            "[38,   600] loss: 0.764\n",
            "[38,   800] loss: 0.774\n",
            "[38,  1000] loss: 0.776\n",
            "[38,  1200] loss: 0.769\n",
            "[38,  1400] loss: 0.740\n",
            "[39,   200] loss: 0.761\n",
            "[39,   400] loss: 0.778\n",
            "[39,   600] loss: 0.752\n",
            "[39,   800] loss: 0.762\n",
            "[39,  1000] loss: 0.779\n",
            "[39,  1200] loss: 0.755\n",
            "[39,  1400] loss: 0.769\n",
            "[40,   200] loss: 0.765\n",
            "[40,   400] loss: 0.755\n",
            "[40,   600] loss: 0.756\n",
            "[40,   800] loss: 0.765\n",
            "[40,  1000] loss: 0.763\n",
            "[40,  1200] loss: 0.743\n",
            "[40,  1400] loss: 0.757\n",
            "[41,   200] loss: 0.720\n",
            "[41,   400] loss: 0.773\n",
            "[41,   600] loss: 0.770\n",
            "[41,   800] loss: 0.750\n",
            "[41,  1000] loss: 0.756\n",
            "[41,  1200] loss: 0.796\n",
            "[41,  1400] loss: 0.775\n",
            "[42,   200] loss: 0.775\n",
            "[42,   400] loss: 0.755\n",
            "[42,   600] loss: 0.746\n",
            "[42,   800] loss: 0.780\n",
            "[42,  1000] loss: 0.738\n",
            "[42,  1200] loss: 0.783\n",
            "[42,  1400] loss: 0.752\n",
            "[43,   200] loss: 0.731\n",
            "[43,   400] loss: 0.742\n",
            "[43,   600] loss: 0.739\n",
            "[43,   800] loss: 0.755\n",
            "[43,  1000] loss: 0.758\n",
            "[43,  1200] loss: 0.753\n",
            "[43,  1400] loss: 0.763\n",
            "[44,   200] loss: 0.765\n",
            "[44,   400] loss: 0.741\n",
            "[44,   600] loss: 0.762\n",
            "[44,   800] loss: 0.755\n",
            "[44,  1000] loss: 0.734\n",
            "[44,  1200] loss: 0.755\n",
            "[44,  1400] loss: 0.745\n",
            "[45,   200] loss: 0.770\n",
            "[45,   400] loss: 0.735\n",
            "[45,   600] loss: 0.736\n",
            "[45,   800] loss: 0.767\n",
            "[45,  1000] loss: 0.743\n",
            "[45,  1200] loss: 0.749\n",
            "[45,  1400] loss: 0.739\n",
            "[46,   200] loss: 0.740\n",
            "[46,   400] loss: 0.737\n",
            "[46,   600] loss: 0.760\n",
            "[46,   800] loss: 0.736\n",
            "[46,  1000] loss: 0.742\n",
            "[46,  1200] loss: 0.742\n",
            "[46,  1400] loss: 0.788\n",
            "[47,   200] loss: 0.758\n",
            "[47,   400] loss: 0.735\n",
            "[47,   600] loss: 0.734\n",
            "[47,   800] loss: 0.733\n",
            "[47,  1000] loss: 0.741\n",
            "[47,  1200] loss: 0.728\n",
            "[47,  1400] loss: 0.733\n",
            "[48,   200] loss: 0.740\n",
            "[48,   400] loss: 0.764\n",
            "[48,   600] loss: 0.741\n",
            "[48,   800] loss: 0.746\n",
            "[48,  1000] loss: 0.727\n",
            "[48,  1200] loss: 0.727\n",
            "[48,  1400] loss: 0.750\n",
            "[49,   200] loss: 0.748\n",
            "[49,   400] loss: 0.727\n",
            "[49,   600] loss: 0.748\n",
            "[49,   800] loss: 0.746\n",
            "[49,  1000] loss: 0.750\n",
            "[49,  1200] loss: 0.706\n",
            "[49,  1400] loss: 0.736\n",
            "[50,   200] loss: 0.730\n",
            "[50,   400] loss: 0.733\n",
            "[50,   600] loss: 0.749\n",
            "[50,   800] loss: 0.756\n",
            "[50,  1000] loss: 0.742\n",
            "[50,  1200] loss: 0.761\n",
            "[50,  1400] loss: 0.762\n",
            "[51,   200] loss: 0.760\n",
            "[51,   400] loss: 0.722\n",
            "[51,   600] loss: 0.778\n",
            "[51,   800] loss: 0.727\n",
            "[51,  1000] loss: 0.745\n",
            "[51,  1200] loss: 0.745\n",
            "[51,  1400] loss: 0.745\n",
            "[52,   200] loss: 0.734\n",
            "[52,   400] loss: 0.746\n",
            "[52,   600] loss: 0.721\n",
            "[52,   800] loss: 0.747\n",
            "[52,  1000] loss: 0.735\n",
            "[52,  1200] loss: 0.731\n",
            "[52,  1400] loss: 0.746\n",
            "[53,   200] loss: 0.707\n",
            "[53,   400] loss: 0.711\n",
            "[53,   600] loss: 0.732\n",
            "[53,   800] loss: 0.717\n",
            "[53,  1000] loss: 0.756\n",
            "[53,  1200] loss: 0.752\n",
            "[53,  1400] loss: 0.732\n",
            "[54,   200] loss: 0.725\n",
            "[54,   400] loss: 0.740\n",
            "[54,   600] loss: 0.730\n",
            "[54,   800] loss: 0.774\n",
            "[54,  1000] loss: 0.751\n",
            "[54,  1200] loss: 0.737\n",
            "[54,  1400] loss: 0.727\n",
            "[55,   200] loss: 0.732\n",
            "[55,   400] loss: 0.705\n",
            "[55,   600] loss: 0.737\n",
            "[55,   800] loss: 0.723\n",
            "[55,  1000] loss: 0.718\n",
            "[55,  1200] loss: 0.713\n",
            "[55,  1400] loss: 0.763\n",
            "[56,   200] loss: 0.698\n",
            "[56,   400] loss: 0.741\n",
            "[56,   600] loss: 0.724\n",
            "[56,   800] loss: 0.715\n",
            "[56,  1000] loss: 0.709\n",
            "[56,  1200] loss: 0.734\n",
            "[56,  1400] loss: 0.731\n",
            "[57,   200] loss: 0.723\n",
            "[57,   400] loss: 0.738\n",
            "[57,   600] loss: 0.740\n",
            "[57,   800] loss: 0.744\n",
            "[57,  1000] loss: 0.748\n",
            "[57,  1200] loss: 0.773\n",
            "[57,  1400] loss: 0.797\n",
            "[58,   200] loss: 0.725\n",
            "[58,   400] loss: 0.726\n",
            "[58,   600] loss: 0.725\n",
            "[58,   800] loss: 0.724\n",
            "[58,  1000] loss: 0.709\n",
            "[58,  1200] loss: 0.736\n",
            "[58,  1400] loss: 0.722\n",
            "[59,   200] loss: 0.747\n",
            "[59,   400] loss: 0.713\n",
            "[59,   600] loss: 0.718\n",
            "[59,   800] loss: 0.736\n",
            "[59,  1000] loss: 0.724\n",
            "[59,  1200] loss: 0.721\n",
            "[59,  1400] loss: 0.737\n",
            "[60,   200] loss: 0.725\n",
            "[60,   400] loss: 0.732\n",
            "[60,   600] loss: 0.711\n",
            "[60,   800] loss: 0.711\n",
            "[60,  1000] loss: 0.731\n",
            "[60,  1200] loss: 0.735\n",
            "[60,  1400] loss: 0.709\n",
            "[61,   200] loss: 0.715\n",
            "[61,   400] loss: 0.743\n",
            "[61,   600] loss: 0.743\n",
            "[61,   800] loss: 0.703\n",
            "[61,  1000] loss: 0.704\n",
            "[61,  1200] loss: 0.752\n",
            "[61,  1400] loss: 0.720\n",
            "[62,   200] loss: 0.705\n",
            "[62,   400] loss: 0.737\n",
            "[62,   600] loss: 0.682\n",
            "[62,   800] loss: 0.738\n",
            "[62,  1000] loss: 0.740\n",
            "[62,  1200] loss: 0.736\n",
            "[62,  1400] loss: 0.728\n",
            "[63,   200] loss: 0.709\n",
            "[63,   400] loss: 0.698\n",
            "[63,   600] loss: 0.715\n",
            "[63,   800] loss: 0.726\n",
            "[63,  1000] loss: 0.698\n",
            "[63,  1200] loss: 0.737\n",
            "[63,  1400] loss: 0.741\n",
            "[64,   200] loss: 0.719\n",
            "[64,   400] loss: 0.702\n",
            "[64,   600] loss: 0.714\n",
            "[64,   800] loss: 0.704\n",
            "[64,  1000] loss: 0.727\n",
            "[64,  1200] loss: 0.735\n",
            "[64,  1400] loss: 0.716\n",
            "[65,   200] loss: 0.730\n",
            "[65,   400] loss: 0.691\n",
            "[65,   600] loss: 0.702\n",
            "[65,   800] loss: 0.710\n",
            "[65,  1000] loss: 0.711\n",
            "[65,  1200] loss: 0.711\n",
            "[65,  1400] loss: 0.726\n",
            "[66,   200] loss: 0.723\n",
            "[66,   400] loss: 0.717\n",
            "[66,   600] loss: 0.742\n",
            "[66,   800] loss: 0.708\n",
            "[66,  1000] loss: 0.724\n",
            "[66,  1200] loss: 0.709\n",
            "[66,  1400] loss: 0.718\n",
            "[67,   200] loss: 0.724\n",
            "[67,   400] loss: 0.727\n",
            "[67,   600] loss: 0.702\n",
            "[67,   800] loss: 0.707\n",
            "[67,  1000] loss: 0.736\n",
            "[67,  1200] loss: 0.731\n",
            "[67,  1400] loss: 0.753\n",
            "[68,   200] loss: 0.723\n",
            "[68,   400] loss: 0.692\n",
            "[68,   600] loss: 0.756\n",
            "[68,   800] loss: 0.725\n",
            "[68,  1000] loss: 0.717\n",
            "[68,  1200] loss: 0.746\n",
            "[68,  1400] loss: 0.718\n",
            "[69,   200] loss: 0.731\n",
            "[69,   400] loss: 0.706\n",
            "[69,   600] loss: 0.711\n",
            "[69,   800] loss: 0.742\n",
            "[69,  1000] loss: 0.697\n",
            "[69,  1200] loss: 0.746\n",
            "[69,  1400] loss: 0.728\n",
            "[70,   200] loss: 0.700\n",
            "[70,   400] loss: 0.699\n",
            "[70,   600] loss: 0.684\n",
            "[70,   800] loss: 0.708\n",
            "[70,  1000] loss: 0.720\n",
            "[70,  1200] loss: 0.732\n",
            "[70,  1400] loss: 0.705\n",
            "[71,   200] loss: 0.707\n",
            "[71,   400] loss: 0.724\n",
            "[71,   600] loss: 0.703\n",
            "[71,   800] loss: 0.723\n",
            "[71,  1000] loss: 0.700\n",
            "[71,  1200] loss: 0.742\n",
            "[71,  1400] loss: 0.740\n",
            "[72,   200] loss: 0.710\n",
            "[72,   400] loss: 0.706\n",
            "[72,   600] loss: 0.705\n",
            "[72,   800] loss: 0.707\n",
            "[72,  1000] loss: 0.690\n",
            "[72,  1200] loss: 0.738\n",
            "[72,  1400] loss: 0.746\n",
            "[73,   200] loss: 0.705\n",
            "[73,   400] loss: 0.711\n",
            "[73,   600] loss: 0.691\n",
            "[73,   800] loss: 0.743\n",
            "[73,  1000] loss: 0.705\n",
            "[73,  1200] loss: 0.699\n",
            "[73,  1400] loss: 0.736\n",
            "[74,   200] loss: 0.702\n",
            "[74,   400] loss: 0.705\n",
            "[74,   600] loss: 0.723\n",
            "[74,   800] loss: 0.723\n",
            "[74,  1000] loss: 0.697\n",
            "[74,  1200] loss: 0.687\n",
            "[74,  1400] loss: 0.732\n",
            "[75,   200] loss: 0.716\n",
            "[75,   400] loss: 0.718\n",
            "[75,   600] loss: 0.712\n",
            "[75,   800] loss: 0.727\n",
            "[75,  1000] loss: 0.690\n",
            "[75,  1200] loss: 0.700\n",
            "[75,  1400] loss: 0.696\n",
            "[76,   200] loss: 0.734\n",
            "[76,   400] loss: 0.694\n",
            "[76,   600] loss: 0.703\n",
            "[76,   800] loss: 0.724\n",
            "[76,  1000] loss: 0.694\n",
            "[76,  1200] loss: 0.698\n",
            "[76,  1400] loss: 0.710\n",
            "[77,   200] loss: 0.731\n",
            "[77,   400] loss: 0.709\n",
            "[77,   600] loss: 0.726\n",
            "[77,   800] loss: 0.728\n",
            "[77,  1000] loss: 0.689\n",
            "[77,  1200] loss: 0.706\n",
            "[77,  1400] loss: 0.726\n",
            "[78,   200] loss: 0.674\n",
            "[78,   400] loss: 0.684\n",
            "[78,   600] loss: 0.684\n",
            "[78,   800] loss: 0.705\n",
            "[78,  1000] loss: 0.702\n",
            "[78,  1200] loss: 0.749\n",
            "[78,  1400] loss: 0.716\n",
            "[79,   200] loss: 0.692\n",
            "[79,   400] loss: 0.686\n",
            "[79,   600] loss: 0.724\n",
            "[79,   800] loss: 0.707\n",
            "[79,  1000] loss: 0.717\n",
            "[79,  1200] loss: 0.731\n",
            "[79,  1400] loss: 0.702\n",
            "[80,   200] loss: 0.721\n",
            "[80,   400] loss: 0.700\n",
            "[80,   600] loss: 0.736\n",
            "[80,   800] loss: 0.696\n",
            "[80,  1000] loss: 0.663\n",
            "[80,  1200] loss: 0.697\n",
            "[80,  1400] loss: 0.677\n",
            "[81,   200] loss: 0.679\n",
            "[81,   400] loss: 0.691\n",
            "[81,   600] loss: 0.699\n",
            "[81,   800] loss: 0.716\n",
            "[81,  1000] loss: 0.684\n",
            "[81,  1200] loss: 0.740\n",
            "[81,  1400] loss: 0.709\n",
            "[82,   200] loss: 0.729\n",
            "[82,   400] loss: 0.682\n",
            "[82,   600] loss: 0.706\n",
            "[82,   800] loss: 0.699\n",
            "[82,  1000] loss: 0.681\n",
            "[82,  1200] loss: 0.696\n",
            "[82,  1400] loss: 0.698\n",
            "[83,   200] loss: 0.705\n",
            "[83,   400] loss: 0.699\n",
            "[83,   600] loss: 0.690\n",
            "[83,   800] loss: 0.689\n",
            "[83,  1000] loss: 0.716\n",
            "[83,  1200] loss: 0.704\n",
            "[83,  1400] loss: 0.686\n",
            "[84,   200] loss: 0.724\n",
            "[84,   400] loss: 0.724\n",
            "[84,   600] loss: 0.669\n",
            "[84,   800] loss: 0.699\n",
            "[84,  1000] loss: 0.722\n",
            "[84,  1200] loss: 0.727\n",
            "[84,  1400] loss: 0.688\n",
            "[85,   200] loss: 0.715\n",
            "[85,   400] loss: 0.709\n",
            "[85,   600] loss: 0.715\n",
            "[85,   800] loss: 0.696\n",
            "[85,  1000] loss: 0.702\n",
            "[85,  1200] loss: 0.665\n",
            "[85,  1400] loss: 0.688\n",
            "[86,   200] loss: 0.697\n",
            "[86,   400] loss: 0.715\n",
            "[86,   600] loss: 0.699\n",
            "[86,   800] loss: 0.701\n",
            "[86,  1000] loss: 0.722\n",
            "[86,  1200] loss: 0.694\n",
            "[86,  1400] loss: 0.699\n",
            "[87,   200] loss: 0.693\n",
            "[87,   400] loss: 0.682\n",
            "[87,   600] loss: 0.714\n",
            "[87,   800] loss: 0.687\n",
            "[87,  1000] loss: 0.705\n",
            "[87,  1200] loss: 0.697\n",
            "[87,  1400] loss: 0.701\n",
            "[88,   200] loss: 0.660\n",
            "[88,   400] loss: 0.714\n",
            "[88,   600] loss: 0.688\n",
            "[88,   800] loss: 0.720\n",
            "[88,  1000] loss: 0.702\n",
            "[88,  1200] loss: 0.689\n",
            "[88,  1400] loss: 0.722\n",
            "[89,   200] loss: 0.711\n",
            "[89,   400] loss: 0.683\n",
            "[89,   600] loss: 0.709\n",
            "[89,   800] loss: 0.718\n",
            "[89,  1000] loss: 0.684\n",
            "[89,  1200] loss: 0.699\n",
            "[89,  1400] loss: 0.679\n",
            "[90,   200] loss: 0.707\n",
            "[90,   400] loss: 0.690\n",
            "[90,   600] loss: 0.700\n",
            "[90,   800] loss: 0.697\n",
            "[90,  1000] loss: 0.708\n",
            "[90,  1200] loss: 0.701\n",
            "[90,  1400] loss: 0.708\n",
            "[91,   200] loss: 0.656\n",
            "[91,   400] loss: 0.695\n",
            "[91,   600] loss: 0.677\n",
            "[91,   800] loss: 0.696\n",
            "[91,  1000] loss: 0.707\n",
            "[91,  1200] loss: 0.726\n",
            "[91,  1400] loss: 0.729\n",
            "[92,   200] loss: 0.662\n",
            "[92,   400] loss: 0.721\n",
            "[92,   600] loss: 0.705\n",
            "[92,   800] loss: 0.677\n",
            "[92,  1000] loss: 0.681\n",
            "[92,  1200] loss: 0.690\n",
            "[92,  1400] loss: 0.719\n",
            "[93,   200] loss: 0.695\n",
            "[93,   400] loss: 0.689\n",
            "[93,   600] loss: 0.688\n",
            "[93,   800] loss: 0.727\n",
            "[93,  1000] loss: 0.680\n",
            "[93,  1200] loss: 0.704\n",
            "[93,  1400] loss: 0.676\n",
            "[94,   200] loss: 0.721\n",
            "[94,   400] loss: 0.705\n",
            "[94,   600] loss: 0.676\n",
            "[94,   800] loss: 0.725\n",
            "[94,  1000] loss: 0.703\n",
            "[94,  1200] loss: 0.676\n",
            "[94,  1400] loss: 0.694\n",
            "[95,   200] loss: 0.684\n",
            "[95,   400] loss: 0.690\n",
            "[95,   600] loss: 0.683\n",
            "[95,   800] loss: 0.724\n",
            "[95,  1000] loss: 0.704\n",
            "[95,  1200] loss: 0.693\n",
            "[95,  1400] loss: 0.713\n",
            "[96,   200] loss: 0.673\n",
            "[96,   400] loss: 0.681\n",
            "[96,   600] loss: 0.693\n",
            "[96,   800] loss: 0.701\n",
            "[96,  1000] loss: 0.699\n",
            "[96,  1200] loss: 0.713\n",
            "[96,  1400] loss: 0.671\n",
            "[97,   200] loss: 0.691\n",
            "[97,   400] loss: 0.706\n",
            "[97,   600] loss: 0.695\n",
            "[97,   800] loss: 0.660\n",
            "[97,  1000] loss: 0.737\n",
            "[97,  1200] loss: 0.661\n",
            "[97,  1400] loss: 0.674\n",
            "[98,   200] loss: 0.665\n",
            "[98,   400] loss: 0.698\n",
            "[98,   600] loss: 0.704\n",
            "[98,   800] loss: 0.676\n",
            "[98,  1000] loss: 0.685\n",
            "[98,  1200] loss: 0.678\n",
            "[98,  1400] loss: 0.697\n",
            "[99,   200] loss: 0.666\n",
            "[99,   400] loss: 0.716\n",
            "[99,   600] loss: 0.661\n",
            "[99,   800] loss: 0.716\n",
            "[99,  1000] loss: 0.736\n",
            "[99,  1200] loss: 0.670\n",
            "[99,  1400] loss: 0.704\n",
            "[100,   200] loss: 0.708\n",
            "[100,   400] loss: 0.667\n",
            "[100,   600] loss: 0.685\n",
            "[100,   800] loss: 0.705\n",
            "[100,  1000] loss: 0.688\n",
            "[100,  1200] loss: 0.697\n",
            "[100,  1400] loss: 0.702\n",
            "[101,   200] loss: 0.660\n",
            "[101,   400] loss: 0.685\n",
            "[101,   600] loss: 0.710\n",
            "[101,   800] loss: 0.689\n",
            "[101,  1000] loss: 0.706\n",
            "[101,  1200] loss: 0.698\n",
            "[101,  1400] loss: 0.673\n",
            "[102,   200] loss: 0.687\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the modified CNN with additional hidden layers\n",
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)  # Additional fully connected layer\n",
        "        self.fc3 = nn.Linear(256, 128)  # Additional fully connected layer\n",
        "        self.fc4 = nn.Linear(128, 10)   # Final layer adjusted for output\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(self.dropout(x)))\n",
        "        x = F.relu(self.fc2(self.dropout(x)))\n",
        "        x = F.relu(self.fc3(self.dropout(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "net = ExtendedCNN()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training and Testing Functions with 300 epochs\n",
        "def train():\n",
        "    net.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(300):  # Adjust to 300 epochs\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:    # Print every 200 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Finished Training. Total training time: {training_time:.2f} seconds')\n",
        "\n",
        "def test():\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f}%')\n",
        "\n",
        "# Run training and testing\n",
        "train()\n",
        "test()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9+jRwKGrvuFOq2e5Va/Vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}